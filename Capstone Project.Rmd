---
title: "YoTube Sensation"
output:

  md_document:
    variant: markdown_github
---

## Introduction
  Go to the app store on your phone and try to find an app to watch videos and YouTube surfaces as the top-rated. YouTube is the most popular app and website to watch any video.  I often share YouTube videos on lawn care and planting gardens.  YouTube is an American video-sharing website with headquartes in San Bruno, California. Chad Hurley, Steve Chen, and Jawed Karim founded YouTube in February 2005. Google bought the site in November 2006 for US$1.65 billion, and now the largely branded company operates as one of Google's affiliates. YouTube allows you to explore new content, music, news, and more through an official app or the website. Subscribing to particular channels that house the users favorite content, sharing videos with friends, or uploading videos for public scrutiny are some significant features of YouTube. 

  YouTube most popular videos are labeled as "trending." Trending helps viewers to see the top news, events, and miscellaneous situations in the video world. Trending videos are collected primarily when a wide range of viewers would find interesting. Some trends are predictable, like a new song from a famous artist or a new movie trailer. Others are surprising, like a viral video. Trending is not personalized and displays the same list of trending videos in each country to all user. The list of trending videos is updated roughly every 15 minutes. With each update, videos may move up, down, but rarely stay in the same position in the list. YouTube maintains a list of the top trending videos. YouTube uses a combination of factors, including measuring the number of views, shares, comments, and likes to make the commercials and determine trending videos. 

  This dataset is a daily record of the top trending YouTube videos and the factors that influences them.  Analyzing this dataset, we look at the main factors that are included in making the videos trending such as likes, dislikes, and comments. We can view what is needed to make a video be on the trending list. To analyze this data plan is to use linear regression with the variables likes, dislike, comments, and names. The deliverable will be a poster.

  The data set that used comes from kaggle.com the original data set has 40,881 observations and 14 variables.  This data set includes trending videos in 2017 and 2018.  Since I want to become a YouTube personality, I explore information regarding how videos start to trend.  Evaluating, the likes, dislikes, views, and comments, the data set seems to have valuable information.  This capstone project encompasses all of the techniques that were learned in this course.  It begins with the data wrangling process - the process of cleaning the data set.  Exploratory data analysis is the next section of the project.  It includes the inferential knowledge that leads to selecting the independent and dependent variables.   In the machine learning section, linear regression models are built to predict the number of views necessary for a video to trend.  The results are noted, and future work is described.
  
  We aim to help YouTube to provide users with a threshold so they can know precisely know what is expected for views, likes, dislikes and comments for their video to be considered as a trending video.  The information gathered and presented provides clear cut information to users.  It also will help YouTube to determine the YouTube channel that they can use for advertisement purposes.    
  
  The First step in for any project is to install and load the necessary packages by declaring the libraries. The plyr package is to be used to select the variables in the clean data and the tidy verse for most of the other functions.  Tidyverse is considered the swiss knife because of its unlimited capabilities.  All packages used in the data wrangling process and the statistics are loaded into the console.
```{r}
library(tidyverse) #The swiss knife!
library(ggplot2)
library(rmarkdown)
library(dplyr)
library(tidyr)
library(knitr)
library(caret)
```

To begin the cleaning process, we take a peek at the working directory with the getwd() function.  If the working directory is not set to read the files, then the working directory has to be changed.  The working directory is confirmed, and the data is imported.

```{r}
getwd()
df <- read.csv("YouTube.csv")
```

```{r}
str(df)
```
The view function gives a table of all observations and variables. Some of the variables were easier to understand than others. The varialbes such as likes, dislikes, comment count, and views were plainly understood. The other variables did not have units specified, which makes it hard to interpret. There are missing variables in the data set labeled as NA.  Let's take a glimpse of the data set.

The glimpse function provides more concise information regarding the data set.  A glimpse of the data set is called to determine the type of variables and ensure that the data set is correct.

```{r}
glimpse(df)
```
The table shows that there is 14 variables and 40,881 observations.  The variables are labelled as factor,integer, and lgl.  


## Data Wrangling
Data wrangling is one of the most essential parts of a data science project. Data wrangling is the process of mapping data from raw to a format appropriate for the data to be analyzed. Data wrangling consists of selecting, indexing, and mutating the tables to form them for your need. This process helps with better decision making, but it takes a long time to complete.  If it is done correctly, then a data scientist life is made easier.   These steps will help to deliver more accurate results.  The data used contained 14 variables and 40,881 observations. They are multiple techniques used to clean and conform the data before the analysis. The packages have been loaded in a previous code chunk.  The loaded packages allow syntax that is simple to use when cleaning the data.  The first step is to remove unnecessary variables which are the columns of the data set.   Once the columns are removed, the data set is reviewed using the glimpse function.


```{r}
df <- df[c(-1,-4,-5,-6,-7,-12,-13,-14,-15,-16)]
glimpse(df)
nrow(df)
```
The table shows that there is 6 variables and 40,881 observations.  The variables are labelled as factor and integer.  By removing insignificant variables from the data frame, the new data set contains the variables trending date, title likes, view, dislikes, and comment count. Theses variable provides information pertinent information regarding the trending videos. 

Next, we determine what to do with missing variables.  There are several ways to handle the missing variables.  Since the data is large, we omit the variables with missing data. To do this NA is changed to 0 and then 0 is omitted them. The omit function is used to remove the variable with missing data values. 

```{r}
df[df == 0] <- NA
df <- na.omit(df)
glimpse(df)
```

Since the missing data values have been deleted the number of observations are minimized.  The table shows that there is 6 variables and 39,924 observations.  The variables are labelled as factor and integer.  The data is numerical. Thus we look at a summary of the data.

```{r}
mean(df$views)
summary(df$views)
```

The mean number of views is 1,153,527.  The data set has a minimum value of 1023 and a maximum value of 137,843,120.  The range of these values is large.  The value 1023 doesn't seem logical for a trending video.  Thus a filter is created to include only observations at a certain threshold.  This threshold is 148,818, the mean of the first quartile. The new dat frame is called df_1.

```{r}
df_1<-df %>% filter(views>=148818)
glimpse(df_1)
```

The table shows that the data set df_1 has 6 variables and 29,943 observations.  Let's take a look at the summary of df_1 to view the changes in the mean number of views.


```{r}
summary(df_1$views)
```

There is evidence that some trending videos are more popular than others.  A popularity index is created to distinguish between highly trending and trending videos based on the number of views. The threshold is selected to be 586,076 the medeian number of views.

```{r}
df_1$Popularity_Index <- ifelse(( df_1$views >= 586076), '1', '0')
```

We take one last look at the clean data set and write it to the console.


```{r}
glimpse(df_1)
write.csv(df_1, file = "df_1clean.csv")
```

The clean data set has 7 variables and 29,943 observations.  The clean data set is called df_1clean.


## Statistical Analysis

Now that the data set is cleaned, it is time for the statistical analysis. This analysis is the exploratory portion of the project. At this time, we are trying to find trends in the project data based on the facts and develop a solid hypothesis. In this portion of the coding in R, plots are created and analyzed. A deep analysis of the trending videos is investigated.


The necessary libraries needed for the Statistical Analysis portion were installed and are loaded here.



All of the packages are loaded.  Therefore, we load the data into R and call the data frame dta.

```{r}

df_1clean <- read.csv("df_1clean.csv")

```



When reading in the data file, an additional variable appears.  That variable is removed.  A glimpse of the data set is called to determine the type of variables and ensure that the data set is correct.
```{r}

df_1clean <- select(df_1clean, -c(X))
glimpse(df_1clean)

```


The table shows that there is 7 variables and 29,943 observations.  The variables are labelled as factor and integer.  

Since there is an enormous amount of data for trending videos, the means of each variable is explored.  The code to find the means is written below.



```{r}

daily_means <- df_1clean  %>%
  group_by(trending_date) %>%
  summarise(n_title = n(),
            mean_likes = mean(likes),
             mean_views = mean(views),
             mean_dislikes = mean(dislikes),
            mean_comment_count = mean(comment_count))
daily_means

```

The mean for the variables, likes, dislikes, views, and comment_count is collect for each trending date.  The table also shows the number of videos per date.  A graphical analysis of the means is annotated in the code below.  We conduct this anlysis as it reduces the number of data points and patterns can be seen.  


```{r}

ggplot(daily_means, aes(y = mean_views, x = mean_likes, color=mean_dislikes)) + geom_point() + geom_jitter()

ggplot(daily_means, aes(y = mean_views, x = mean_likes, color=(mean_comment_count))) + geom_point() + geom_jitter()

ggplot(daily_means, aes(y = mean_views, x = mean_comment_count,  color=mean_dislikes)) + geom_point() + geom_jitter()

ggplot(daily_means, aes(y = mean_views, x = mean_dislikes,color=mean_comment_count)) + geom_point() + geom_jitter()

```
The plots show strong positive relationships between the means of the independent and dependent variables.  Thus there may be a strong correlation between the actual variables.  Note that a trending video that has an extremely large number of likes and views it also has a vast amount of dislikes and comments.  Plots for additional anlysis betwen the means are below.  The ralations hip beteen the variables is strong.  


```{r}

ggplot(daily_means, aes(y = mean_dislikes, x = mean_likes, color=(mean_comment_count))) + geom_point() + geom_jitter()

ggplot(daily_means, aes(x =mean_likes , y = mean_comment_count, )) + geom_point() + geom_jitter()

ggplot(daily_means, aes(y = mean_dislikes, x =mean_comment_count , )) + geom_point() + geom_jitter()

```



```{r}
ggplot(df_1clean, aes(y = views, x = likes, color=dislikes)) + geom_point() + geom_jitter()

ggplot(df_1clean, aes(y = views, x = likes, color=comment_count)) + geom_point() + geom_jitter()

ggplot(df_1clean, aes(y = views, x = comment_count, color=dislikes)) + geom_point() + geom_jitter()

ggplot(df_1clean, aes(y = views, x = dislikes, color=comment_count)) + geom_point() + geom_jitter()
```


To the naked eye, the relationship between the actual variables does not appear to be as strong.   A large amount of the data is clumped around the (0,0) point.  Nevertheless, there is a positive relationship between the variables in the clean data set.  It is worthwhile to note that videos that have a low number of dislikes and a moderately high number of views have high levels of comments.  Lets view the box plots for the clean data set.


```{r}
bp <- ggplot(df_1clean, aes(x=Popularity_Index, y=likes, fill=Popularity_Index)) + 
  geom_boxplot()+
  labs(title="Plot of Views Given Likes",x="Popularity Index", y = "Number of Views")+
scale_x_discrete(limits=c("Popular", "Highly Popular"))
bp + theme_classic()
```

There are too many outliers to use a box plot :( .  I leave it to show the complete anlysis that was conducted.



Let's take a look at the Popularity_Index variable that was created.
```{r}
df_1clean$Popularity_Index <- ifelse(df_1clean$Popularity_Index == 1, 'Highly Popular', 'Popular')
ggplot(df_1clean, aes(x = likes, y = views, color=Popularity_Index)) +
  geom_point() + geom_jitter()

ggplot(df_1clean, aes(x = dislikes, y = views, color=Popularity_Index)) +
  geom_point() + geom_jitter()

ggplot(df_1clean, aes(x = comment_count, y = views, color=Popularity_Index)) +
  geom_point() + geom_jitter()
```

Recall that the variables Popularity _Index was determined by the mean of views in the original data set.  It seems as if the clean data set includes the majority of highly popular videos.  Looking at the plots there are only a few popular trending videos as indicated by the small splash of mint green.


```{r}
 hist_plot <-
   df_1clean %>%
     ggplot(aes(x = views)) +
     geom_histogram(color = "maroon", fill = "dodgerblue", bins = 205)
  print(hist_plot)
  
  hist_plot <-
   df_1clean %>%
     ggplot(aes(x = likes)) +
     geom_histogram(color = "maroon", fill = "dodgerblue", bins = 205)
  print(hist_plot)
  
  hist_plot <-
   df_1clean %>%
     ggplot(aes(x = dislikes)) +
     geom_histogram(color = "maroon", fill = "dodgerblue", bins = 205)
  print(hist_plot)
  
  hist_plot <-
   df_1clean %>%
     ggplot(aes(x = comment_count)) +
     geom_histogram(color = "maroon", fill = "dodgerblue", bins = 205)
  print(hist_plot)
  
  
```
In the histograms, 205 bins were used because there were 205 dates.  The histograms generally have the same shape. The number of dislikes is fewer than any other variable. The variable views have a more extensive amount spread. Additonal histrograms are shown below.


```{r}

dta_hist <- ggplot(df_1clean, aes(likes))
dta_hist + geom_bar(aes(fill=views), width = 0.6, col="blue") + theme(axis.text.x =     element_text(angle=65, vjust=0.6)) + labs(title="Views Given Likes")



dta_hist <- ggplot(df_1clean, aes(dislikes))
dta_hist + geom_bar(aes(fill=views), width = 0.6, col="blue") + theme(axis.text.x = element_text(angle=65, vjust=0.6)) + labs(title="Views Given Dislikes")

dta_hist <- ggplot(df_1clean, aes(comment_count))
dta_hist + geom_bar(aes(fill=views), width = 0.6, col="blue") + theme(axis.text.x =     element_text(angle=65, vjust=0.6)) + labs(title="Views Given Comments")


```

The histograms above allow the reader to see the outliers more easily.  Since views are the dependent variable, the histograms are constructed to count the number of views given the number of likes, dislikes, and comments, respectively.  The y-axis is in 10,000.

We conclude, by showing the top 10 trending dates for the data set.  It is determined the May 19, 2018 is the winner.


```{r}

df_1clean %>% count(trending_date) %>% arrange(desc(n)) %>%

  top_n(10) %>%

  ggplot(aes(x = reorder(trending_date, n), y = n, fill = n)) +

  geom_bar(stat = 'identity') +

  coord_flip() +

  labs(x = "Date", y = "Number of Trending Videos", title="Trending Views By Date")

```

Next, we calculate the differences in means for the covariates likes, dislikes, and comments>



```{r}
cov <- c( "likes", "dislikes", "comment_count")
df_1clean %>%
 # group_by(Treatment) %>%
  select(one_of(cov)) %>%
  summarise_all(funs(mean))
```


The next code shows a hypothesis test.

```{r}
lapply(cov, function(v) {
    t.test(df_1clean[, v] ~ df_1clean$Popularity_Index)})

```
A t-test can provide further analysis. Compute a t.test to determine if the difference in means is statistically significant at conventional levels of confidence.   If p is larger than 0.05, accept the null that the two means are equal.  For a smaller p-value, more confidence is given when rejecting the null hypothesis.  Since $p<0.05$ the null.  According, to the summary table we reject the nul hypotheses.

Recall:  Null hypotheses - the mean of the two samples are equal.    



```{r}
write.csv(daily_means, file = "daily_means.csv")
```

## Machine Learning
The plots and models are created, and a clearer understanding of the data set has been achieved. Since the dependent variable is a numerical value, multiple linear regression will be used as the machine learning technique. This model explores the relationship between the dependent variables and the independent variables.  The dependent variable is views, and the independent variables are likes, dislikes, and comments.  For the machine learning portion of the capstone project, the number of views is predicted given the number of likes. 

Before the linear model is created, the data is divided into testing and training data.  The divide used is 80% to train the model and 20% of the data to test the model.  To determine if the model of best fit is  a good model the metrics R-squared RMSE, and p-value are observed. The model is built using the original data and not the daily means of the the data.  The code below splits the data set.  Set seed gives a starting point for splitting the data.

```{r}
set.seed(7)
validation_index <- createDataPartition(df_1clean$views, p=0.80, list=FALSE)
training <- df_1clean[-validation_index,]
# use the remaining 80% of data to training and testing the models
testing <- df_1clean[validation_index,]

```

The first model is creating with views as the dependent variable and likes, dislikes, an comments as the depedendent variable.

```{r}
model4 <- lm(views ~ dislikes + comment_count + likes, data = testing)
summary(model4)
```


## Model 4
This is the fourth model and it is the model of best fit.
$$ y=419200+55.39(dislikes)-37.72(comment\_count)+22.68(likes)$$
We are interested in whether the total number of views for a trending video can be predicted given the number of likes and dislikes.  
The p-value helps to determine if each independent variable is statistically significant.  A p-value is statistically significant and will be indicated by a darkened dot, *, **, or *** if the p-value is between 0 and 0.01.  Typically a p-value should be between $0$ and $0.05$, for a 95% confidence interval.  The independent variables, likes, dislikes, and comment_count in the summary table have p values in the following interval  $0<p-value<0.01$.  Thus these variables are high statistically significant.

We now address the quality of the linear regression fit using $R^2$ and the residual standard error (RSE).  The symbol from RSE is "$\sigma^2$."  The RSE is an estimate of standard error deviation.  That is, it is the average deviation from the true regression line.  In the resulting output, the RSE is $1920000$.  This value reflects that the actual number of views deviates from the true regression line an average of approximately $1920000$ views. The RSE indicates that the model has variability in fitting the data.

The $R^2$ statistic provides an alternative measure of fit.  It measures the proportion of the variability in $Y$ that can be explained using $X$.  The $R^2$ statistic takes on a value, $0\leq R^2\leq 1$.  An $R^2$ value near one indicates that the regression has explained a large proportion of the variability in the response.  In the outcome summary for the model, $R^2=0.7293$.  Since this value is close to $1$, we can conclude that the regression did explain much of the variability in the number of views.  

Information from the $R^2$ and $\sigma^2$ statistics determine that the model was a good fit but has room to be better.  To improve this model, we consider using a different depedent variable.  Selecting additional independent variables and/or removing independent variables may have a significant impact on the model.


```{r}
model5 <- lm(likes ~ dislikes + comment_count + views, data = testing)
summary(model5)
```
```
Running the same model on the training data set shows that $R^2=0.85$ and the RSE is significantly lower.  This shows that there is extremely slight inconsistency in the training and testing models.  In both models the indepedent variables have the same level of significance.

## Recommendations

1. The model is a great predicator for determining the number of views given likes, dislikes, and comments. However becasue there is some inconsitency with the model for the training and testing data set, the counter for the code to split the data set should be changed.  This could give a more reliable training and testing data set. 
2. Use classification models to make sense of the more massive amount of categorical data in the original data set instead of removing the data. Significant infromation can be determined from all aspects of the data in the data set.
3. Perform the same analysis to determine if the data is consistent for 2019.  Performing the same test on different years can determine if the independent variables, likes, dislikes, and comments are still statistically significant.  We can also view the $R^2$ value to learn if the independent variables are still good predictors of views-dependent variable.


## Future Work

What will happen if the model is built for categorical values?  Will the data stil be a perfect match? These are all questions availabe for fututre analysis. Building more efficient models and developing sound reasoning for selecting covariates for the models is an aspect that can enhance this project. Using likes, dislikes, or comments as a dependent variable instead of views may have a significant impact on the model.





